<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>Rohan Paleja | Publications</title>
    <meta name="description" content="Personal Academic Webpage for Rohan Paleja">

    <!-- Fonts and Icons -->
    <link rel="stylesheet" type="text/css"
          href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"/>

    <!-- CSS Files -->
    <link rel="stylesheet" href="/assets/css/all.min.css">
    <link rel="stylesheet" href="/assets/css/academicons.min.css">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="/publications/">

    <!-- Scripts -->
    <script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
    <script src="./main.js"></script>
    <script src="./scroll.js"></script>


    <style>
        .search-container {
            margin: 20px auto;
            max-width: 600px;
        }

        .search-box {
            width: 100%;
            max-width: 600px;
            padding: 8px 12px;
            font-size: 16px;
            border: 2px solid #ddd;
            border-radius: 4px;
            transition: border-color 0.2s ease;
        }

        .search-box:focus {
            outline: none;
            border-color: #666;
        }

        .publication-hidden {
            display: none !important;
        }
    </style>

    <SCRIPT LANGUAGE="JavaScript">

        <!--
        Begin
        var scripts = document.getElementsByTagName('script');
        var myScript = scripts[scripts.length - 1];

        var queryString = myScript.src.replace(/^[^\?]+\??/, '');

        var params = parseQuery(queryString);

        var recruit = 0;

        function parseQuery(query) {
            var Params = {};
            if (!query) return Params;
            var Pairs = query.split(/[;&]/);
            for (var i = 0; i < Pairs.length; i++) {
                var KeyVal = Pairs[i].split('=');
                if (!KeyVal || KeyVal.length != 2) continue;
                var key = unescape(KeyVal[0]);
                var val = unescape(KeyVal[1]);
                val = val.replace(/\+/g, ' ');
                Params[key] = val;
            }
            return Params;
        }


        function showRecruit() {
            if (recruit == 0) {
                document.getElementById('recruit').style = 'display:inline-block';
            } else {
                document.getElementById('recruit').style = 'display:none';
            }
            recruit = 1 - recruit;
        }

        function showPubs(id) {
            if (id == 0) {
                document.getElementById('pubs').innerHTML = document.getElementById('pubs_selected').innerHTML;
                document.getElementById('select0').style = 'text-decoration:underline;color:#000000';
                document.getElementById('select1').style = '';
                document.getElementById('select2').style = '';
                _altmetric_embed_init();
            } else if (id == 1) {
                document.getElementById('pubs').innerHTML = document.getElementById('pubs_by_date').innerHTML;
                document.getElementById('select1').style = 'text-decoration:underline;color:#000000';
                document.getElementById('select0').style = '';
                document.getElementById('select2').style = '';
                _altmetric_embed_init();
            } else {
                document.getElementById('pubs').innerHTML = document.getElementById('pubs_by_topic').innerHTML;
                document.getElementById('select2').style = 'text-decoration:underline;color:#000000';
                document.getElementById('select0').style = '';
                document.getElementById('select1').style = '';
                _altmetric_embed_init();
            }
        }

        //  End -->
    </script>
</head>
<body>
<!-- Header -->
<nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">

        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Rohan</span>
            Paleja</a>
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center" style="line-height: 1em;">
            <a href="mailto:rohan.paleja@mit.edu"><i class="fa fa-envelope-square gm-icon"></i></a>
            <a href="https://scholar.google.com/citations?user=xjnQbKgAAAAJ&hl=en" target="_blank"
               title="Google Scholar"><i class="ai ai-google-scholar-square gs-icon"></i></a>
            <a href="https://github.com/rohanpaleja27" target="_blank" title="GitHub"><i
                    class="fab fa-github-square gh-icon"></i></a>
            <a href="https://www.linkedin.com/in/rohan-paleja-6370a3111/" target="_blank" title="LinkedIn"><i
                    class="fab fa-linkedin li-icon"></i></a>
            <a href="https://twitter.com/rohanpaleja27" target="_blank" title="Twitter"><i
                    class="fab fa-twitter-square tw-icon"></i></a>
          </span>
        </div>

        <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">
                <li class="nav-item ">
                    <a class="nav-link" href="/">
                        About

                    </a>
                </li>

                <li class="nav-item ">
                    <a class="nav-link" href="/assets/pdf/vitae.pdf">
                        Curriculum Vitae
                    </a>
                </li>


                <li class="nav-item ">
                    <a class="nav-link" href="/projects/">
                        Projects

                    </a>
                </li>


                <li class="nav-item navbar-active font-weight-bold">
                    <a class="nav-link" href="/publications/">
                        Publications

                        <span class="sr-only">(current)</span>

                    </a>
                </li>

                <!--              <li class="nav-item ">-->
                <!--                  <a class="nav-link" href="/teaching/">-->
                <!--                    Teaching-->
                <!--                    -->
                <!--                  </a>-->
                <!--              </li>-->

                <!--              <li class="nav-item ">-->
                <!--                  <a class="nav-link" href="/service/">-->
                <!--                    Service-->
                <!--                                        -->
                <!--                  </a>-->
                <!--              </li>-->

            </ul>
        </div>
    </div>
</nav>

<!-- Scrolling Progress Bar -->
<progress id="progress" value="0">
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>
</progress>

<!-- Content -->
<div class="content">

    <h3>Publications</h3>
    <input type="text" id="publicationSearch" placeholder="Search publications..." class="search-box">
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const searchBox = document.getElementById('publicationSearch');

            searchBox.addEventListener('input', function (e) {
                const searchTerm = e.target.value.toLowerCase();

                // Get all publication rows
                const publications = document.querySelectorAll('.row.m-0.mt-3.p-0');

                publications.forEach(pub => {
                    // Search in title, authors, venue, and abstract
                    const title = pub.querySelector('.title')?.textContent.toLowerCase() || '';
                    const author = pub.querySelector('.author')?.textContent.toLowerCase() || '';
                    const venue = pub.querySelector('div')?.textContent.toLowerCase() || '';
                    const abstract = pub.querySelector('.abstract')?.textContent.toLowerCase() || '';

                    const matches = title.includes(searchTerm) ||
                        author.includes(searchTerm) ||
                        venue.includes(searchTerm) ||
                        abstract.includes(searchTerm);

                    // Toggle visibility
                    pub.classList.toggle('publication-hidden', !matches);

                    // Also handle the year headers
                    const yearSection = pub.closest('.row.m-0.p-0');
                    if (yearSection) {
                        const visiblePubs = Array.from(yearSection.querySelectorAll('.row.m-0.mt-3.p-0'))
                            .filter(p => !p.classList.contains('publication-hidden'));

                        // Hide year section if no visible publications
                        const yearHeader = yearSection.querySelector('.bibliography-year')?.closest('.row');
                        if (yearHeader) {
                            yearHeader.classList.toggle('publication-hidden', visiblePubs.length === 0);
                        }
                    }
                });
            });
        });
    </script>

    <h6>
        <nobr><em>*</em></nobr>
        denotes equal contribution and joint lead authorship.
    </h6>
    <p>Blue - Conference Papers. <br>
        Red - Workshop and Doctoral Consortia Papers.<br>
        Orange - Journal Papers.</p>

    <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
        <div class="col-sm-1 mt-2 p-0 pr-1">
            <h3 class="bibliography-year">2025</h3>
        </div>
        <div class="col-sm-11 p-0">
            <ol class="bibliography">

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://neurips.cc" target="_blank">
                                ICLR
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="sreeramdassICLR" class="col p-0">
                                <h5 class="title mb-0">Generalized Behavior Learning from Diverse Demonstrations </h5>
                                <div class="author">
                                    <nobr>Varshith Sreeramdass,</nobr>
                                        <nobr><em>Rohan Paleja</em>,</nobr>
                                        <nobr>Letian Chen,</nobr>
                                        <nobr>Sanne van Waveren,</nobr>
                                        and
                                        <nobr>Matthew Gombolay
                                        </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        International Conference on Learning Representations (ICLR), 2025.
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#sreeramdassICLR-abstract" role="button" aria-expanded="false"
                                       aria-controls="sreeramdassICLR-abstract">Abstract</a>

<!--                                    <a class="badge grey waves-effect font-weight-light mr-1"-->
<!--                                       href="https://openreview.net/pdf?id=LXz1xIEBkF" target="_blank">PDF</a>-->

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="sreeramdassICLR-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Diverse behavior policies are valuable in domains requiring quick test-time
                                            adaptation or personalized human-robot interaction. Human demonstrations
                                            provide rich information regarding task objectives and factors that govern
                                            individual behavior variations, which can be used to characterize
                                            \it{useful} diversity and learn diverse performant policies. However, we
                                            show that prior work that builds naive representations of demonstration
                                            heterogeneity fails in generating successful novel behaviors that generalize
                                            over behavior factors. We propose Guided Strategy Discovery (GSD), which
                                            introduces a novel diversity formulation based on a learned task-relevance
                                            measure that prioritizes behaviors exploring modeled latent factors. We
                                            empirically validate across three continuous control benchmarks for
                                            generalizing to in-distribution (interpolation) and out-of-distribution
                                            (extrapolation) factors that GSD outperforms baselines in novel behavior
                                            discovery by 21%. Finally, we demonstrate that GSD can generalize striking
                                            behaviors for table tennis in a virtual testbed while leveraging human
                                            demonstrations collected in the real world.

                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>

            </ol>

        </div>
    </div>

    <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
        <div class="col-sm-1 mt-2 p-0 pr-1">
            <h3 class="bibliography-year">2024</h3>
        </div>
        <div class="col-sm-11 p-0">
            <ol class="bibliography">
                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://neurips.cc/" target="_blank">
                                NeurIPS
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="paleja2024NeurIPS" class="col p-0">
                                <h5 class="title mb-0">Designs for Enabling Collaboration in Human-Machine Teaming via
                                    Interactive and Explainable Systems</h5>
                                <div class="author">

                                    <nobr><em>Rohan Paleja,</em></nobr>
                                    <nobr>Michael Munje,</nobr>
                                    <nobr>Kimberlee Chang,</nobr>
                                    <nobr>Reed Jensen,</nobr>
                                    and
                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Neural Information Processing Systems (NeurIPS), 2024
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#paleja2024NeurIPS-abstract" role="button" aria-expanded="false"
                                       aria-controls="paleja2024NeurIPS-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://openreview.net/pdf?id=XrK4JK2jBr" target="_blank">PDF</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://github.com/CORE-Robotics-Lab/Team-Development-with-Transparent-Policies"
                                       target="_blank">Code</a>
                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="paleja2024NeurIPS-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Collaborative robots and machine learning-based virtual agents are
                                            increasingly entering the human workspace with the aim of increasing
                                            productivity and enhancing safety. Despite this, we show in a ubiquitous
                                            experimental domain, Overcooked-AI, that state-of-the-art techniques for
                                            human-machine teaming (HMT), which rely on imitation or reinforcement
                                            learning, are brittle and result in a machine agent that aims to decouple
                                            the machine and human’s actions to act independently rather than in a
                                            synergistic fashion. To remedy this deficiency, we develop HMT approaches
                                            that enable iterative, mixed-initiative team development allowing end-users
                                            to interactively reprogram interpretable AI teammates. Our 50-subject study
                                            provides several findings that we summarize into guidelines. While all
                                            approaches underperform a simple collaborative heuristic (a critical,
                                            negative result for learning-based methods), we find that white-box
                                            approaches supported by interactive modification can lead to significant
                                            team development, outperforming white-box approaches alone, and that
                                            black-box approaches are easier to train and result in better HMT
                                            performance highlighting a tradeoff between explainability and interactivity
                                            versus ease-of-training. Together, these findings present three important
                                            future research directions: 1) Improving the ability to generate
                                            collaborative agents with white-box models, 2) Better learning methods to
                                            facilitate collaboration rather than individualized coordination, and 3)
                                            Mixed-initiative interfaces that enable users, who may vary in ability, to
                                            improve collaboration.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://neurips.cc" target="_blank">
                                NeurIPS
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="hurley2024NeurIPS" class="col p-0">
                                <h5 class="title mb-0">STL: Still Tricky Logic (for System Validation, Even When Showing
                                    Your Work) </h5>
                                <div class="author">
                                    <nobr>Isabelle Hurley,</nobr>
                                        <nobr><em>Rohan Paleja</em>,</nobr>
                                        <nobr>Ashley Suh,</nobr>
                                        <nobr>Jaime D. Pe&ntildea,</nobr>
                                        and

                                        <nobr>Ho Chit Siu
                                        </nobr>


                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Conference on Neural Information Processing Systems (NeurIPS), 2024.
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#hurley2024NeurIPS-abstract" role="button" aria-expanded="false"
                                       aria-controls="hurley2024NeurIPS-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://openreview.net/pdf?id=LXz1xIEBkF" target="_blank">PDF</a>

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="hurley2024NeurIPS-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            As learned control policies become increasingly common in autonomous
                                            systems, there is increasing need to ensure that they are interpretable and
                                            can be checked by human stakeholders. Formal specifications have been
                                            proposed as ways to produce human-interpretable policies for autonomous
                                            systems that can still be learned from examples. Previous work showed that
                                            despite claims of interpretability, humans are unable to use formal
                                            specifications presented in a variety of ways to validate even simple robot
                                            behaviors. This work uses active learning, a standard pedagogical method, to
                                            attempt to improve humans' ability to validate policies in signal temporal
                                            logic (STL). Results show that overall validation accuracy is not high, at
                                            65% \pm 15% (mean \pm standard deviation), and that the three conditions of
                                            no active learning,
                                            active learning, and active learning with feedback do not significantly
                                            differ from each other. Our results suggest that the utility of formal
                                            specifications for human interpretability is still unsupported but point to
                                            other avenues of development which may enable improvements in system
                                            validation.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>
                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold orange darken-1 align-middle" style="width: 65px;"
                               href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8860" target="_blank">
                                T-Ro
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="serarj2024tro" class="col p-0">
                                <h5 class="title mb-0">Heterogeneous Policy Networks for Composite Robot Team
                                    Communication and Coordination</h5>
                                <div class="author">
                                    <nobr>Esmaeil Seraj*,</nobr>
                                        <nobr><em>Rohan Paleja*</em>,</nobr>
                                        <nobr>Luis Pimentel,</nobr>
                                        <nobr>Kin Man Lee,</nobr>
                                        <nobr>Zheyuan Wang,</nobr>
                                        <nobr>Daniel Martin,</nobr>
                                        <nobr>Matthew Sklar,</nobr>
                                        <nobr>John Zhang,</nobr>
                                        <nobr>Zahi Kakish,</nobr>
                                        and
                                        <nobr>Matthew Gombolay.
                                        </nobr>


                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        IEEE Transaction on Robotics, Volume 40, pages 3833 - 3849
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#serarj2024tro-abstract" role="button" aria-expanded="false"
                                       aria-controls="serarj2024tro-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://ieeexplore.ieee.org/abstract/document/10606072/authors#authors"
                                       target="_blank">PDF</a>

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="serarj2024tro-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            High-performing human–human teams learn intelligent and efficient
                                            communication and coordination strategies to maximize their joint utility.
                                            These teams implicitly understand the different roles of heterogeneous team
                                            members and adapt their communication protocols accordingly. Multiagent
                                            reinforcement learning (MARL) has attempted to develop computational methods
                                            for synthesizing such joint coordination–communication strategies, but
                                            emulating heterogeneous communication patterns across agents with different
                                            state, action, and observation spaces has remained a challenge. Without
                                            properly modeling agent heterogeneity, as in prior MARL work that leverages
                                            homogeneous graph networks, communication becomes less helpful and can even
                                            deteriorate the team's performance. In the past, we proposed heterogeneous
                                            policy networks (HetNet) to learn efficient and diverse communication models
                                            for coordinating cooperative heterogeneous teams. In this extended work, we
                                            extend HetNet to support scaling heterogeneous robot teams. Building on
                                            heterogeneous graph-attention networks, we show that HetNet not only
                                            facilitates learning heterogeneous collaborative policies, but also enables
                                            end-to-end training for learning highly efficient binarized messaging. Our
                                            empirical evaluation shows that HetNet sets a new state-of-the-art in
                                            learning coordination and communication strategies for heterogeneous
                                            multiagent teams by achieving an 5.84% to 707.65% performance improvement
                                            over the next-best baseline across multiple domains while simultaneously
                                            achieving a 200× reduction in the required communication bandwidth.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>

            </ol>

        </div>
    </div>


    <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
        <div class="col-sm-1 mt-2 p-0 pr-1">
            <h3 class="bibliography-year">2023</h3>
        </div>
        <div class="col-sm-11 p-0">
            <ol class="bibliography">

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold pink darken-1 align-middle" style="width: 65px;"
                               href="https://sites.bu.edu/mrs2023/" target="_blank">
                                MRS
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="wu2023MRS" class="col p-0">
                                <h5 class="title mb-0">Adversarial Search
                                    and Tracking with Multiagent Reinforcement Learning in Sparsely Observable
                                    Environments</h5>
                                <div class="author">


                                    <nobr><a href="https://core-robotics.gatech.edu/people/zixuan-wu/"
                                             target="_blank">Zixuan Wu</a>,
                                    </nobr>

                                    <nobr><a href="https://xunil17.github.io/" target="_blank">Sean Ye</a>,</nobr>

                                    <nobr><a href="https://www.linkedin.com/in/manishanatarajan" target="_blank">Manisha
                                        Natarajan</a>,
                                    </nobr>

                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Letian Chen</a>,</nobr>


                                    <nobr><em>Rohan Paleja</em>,</nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        International Symposium on Multi-Robot and Multi-Agent Systems (MRS), 2023
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#wu2023MRS-abstract" role="button" aria-expanded="false"
                                       aria-controls="wu2023MRS-abstract">Abstract</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://arxiv.org/abs/2306.11301" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/CORE-Robotics-Lab/ICCT" target="_blank">Code</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="wu2023MRS-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            We study a search and tracking (S&T) problem where a team of dynamic search
                                            agents must collaborate to track an adversarial, evasive agent. The
                                            heterogeneous search team may only have access to a limited number of past
                                            adversary trajectories within a large search space. This problem is
                                            challenging for both model-based searching and reinforcement learning (RL)
                                            methods since the adversary exhibits reactionary and deceptive evasive
                                            behaviors in a large space leading to sparse detections for the search
                                            agents. To address this challenge, we propose a novel Multi-Agent RL (MARL)
                                            framework that leverages the estimated adversary location from our learnable
                                            filtering model. We show that our MARL architecture can outperform all
                                            baselines and achieves a 46% increase in detection rate.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>


                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://ieee-iros.org/" target="_blank">
                                IROS
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="ye2023IROS" class="col p-0">
                                <h5 class="title mb-0">Learning Models of Adversarial Agent Behavior under Partial
                                    Observability</h5>
                                <div class="author">

                                    <nobr><a href="https://xunil17.github.io/" target="_blank">Sean Ye</a>,</nobr>

                                    <nobr><a href="https://www.linkedin.com/in/manishanatarajan" target="_blank">Manisha
                                        Natarajan</a>,
                                    </nobr>

                                    <nobr><a href="https://core-robotics.gatech.edu/people/zixuan-wu/"
                                             target="_blank">Zixuan Wu</a>,
                                    </nobr>

                                    <nobr><em>Rohan Paleja</em>,</nobr>

                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Letian Chen</a>,</nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#ye2023IROS-abstract" role="button" aria-expanded="false"
                                       aria-controls="ye2023IROS-abstract">Abstract</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://arxiv.org/pdf/2306.11168.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/CORE-Robotics-Lab/ICCT" target="_blank">Code</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="ye2023IROS-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            The need for opponent modeling and tracking arises in several real-world
                                            scenarios, such as professional sports, video game design, and
                                            drug-trafficking interdiction. In this work, we present graPh neurAl Network
                                            aDvErsarial MOdeliNg wIth mUtual informMation for modeling the behavior of
                                            an adversarial opponent agent. PANDEMONIUM is a novel graph neural network
                                            (GNN) based approach that uses mutual information maximization as an
                                            auxiliary objective to predict the current and future states of an
                                            adversarial opponent with partial observability. To evaluate PANDEMONIUM, we
                                            design two large-scale, pursuit-evasion domains inspired by real-world
                                            scenarios, where a team of heterogeneous agents is tasked with tracking and
                                            interdicting a single adversarial agent, and the adversarial agent must
                                            evade detection while achieving its own objectives. With the mutual
                                            information formulation, PANDEMONIUM outperforms all baselines in both
                                            domains and achieves 31.68% higher log-likelihood on average for future
                                            adversarial state predictions
                                            across both domains.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold orange darken-1 align-middle" style="width: 65px;"
                               href="https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=7083369" target="_blank">
                                IEEE RA-L
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="zaidi2022WTR" class="col p-0">
                                <h5 class="title mb-0">Athletic Mobile Manipulator System for Robotic Wheelchair
                                    Tennis</h5>
                                <div class="author">


                                    <nobr><a href="https://core-robotics.gatech.edu/people/zulfiqar-zaidi/"
                                             target="_blank">Zulfiqar Zaidi</a>,
                                    </nobr>

                                    <nobr>Daniel Martin*,</nobr>
                                    <nobr>Nathaniel Belles,</nobr>
                                    <nobr>Viacheslav Zakharov,</nobr>
                                    <nobr>Arjun Krishna,</nobr>
                                    <nobr>Kin Man Lee,</nobr>
                                    <nobr>Peter Wagstaff,</nobr>
                                    <nobr>Sumedh Naik,</nobr>
                                    <nobr>Matthew Sklar,</nobr>
                                    <nobr>Sugju Choi,</nobr>
                                    <nobr>Yoshiki Kakehi,</nobr>
                                    <nobr>Ruturaj Patil,</nobr>
                                    <nobr>Divya Mallemadugula,</nobr>
                                    <nobr>Florian Pesce,</nobr>
                                    <nobr>Peter Wilson,</nobr>
                                    <nobr>Wendell Hom,</nobr>
                                    <nobr>Matan Diamond,</nobr>
                                    <nobr>Bryan Zhao,</nobr>
                                    <nobr>Nina Moorman,</nobr>

                                    <nobr><em>Rohan Paleja</em>,</nobr>

                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Letian Chen</a>,</nobr>


                                    <nobr><a href="https://esmaeilseraj09.wixsite.com/home" target="_blank">Esmaeil
                                        Seraj</a>,
                                    </nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        IEEE Robotics and Automation Letters, Volume 8, Issue 4, pages 2245-2252
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#zaidi2023WTR-abstract" role="button" aria-expanded="false"
                                       aria-controls="zaidi2023WTR-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://sites.gatech.edu/core-robotics/files/2023/01/zaidi_wtr.pdf"
                                       target="_blank">PDF</a>

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="zaidi2023WTR-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Athletics are a quintessential and universal expression of humanity. From
                                            French monks who in the 12th century invented jeu de paume, the precursor to
                                            modern lawn tennis, back to the K'iche' people who played the Maya Ballgame
                                            as a form of religious expression over three thousand years ago, humans have
                                            sought to train their minds and bodies to excel in sporting contests.
                                            Advances in robotics are opening up the possibility of robots in sports.
                                            Yet, key challenges remain, as most prior works in robotics for sports are
                                            limited to pristine sensing environments, do not require significant force
                                            generation, or are on miniaturized scales unsuited for joint human-robot
                                            play. In this paper, we propose the first open-source, autonomous robot for
                                            playing regulation wheelchair tennis. We demonstrate the performance of our
                                            full-stack system in executing ground strokes and evaluate each of the
                                            system's hardware and software components. The goal of this paper is to (1)
                                            inspire more research in human-scale robot athletics and (2) establish the
                                            first baseline for a reproducible wheelchair tennis robot for regulation
                                            singles play. Our paper contributes to the science of systems design and
                                            poses a set of key challenges for the robotics community to address in
                                            striving towards robots that can match human capabilities in sports.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>

                </li>

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://humanrobotinteraction.org/2023/" target="_blank">
                                HRI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="krishna2023HRI" class="col p-0">
                                <h5 class="title mb-0">The Effect of Robot Skill Level and Communication in Rapid,
                                    Proximate Human-Robot Collaboration</h5>
                                <div class="author">

                                    <nobr><a href="https://www.linkedin.com/in/kin-man-lee-52b7b681" target="_blank">Kin
                                        Man Lee</a>,
                                    </nobr>

                                    <nobr><a href="https://www.linkedin.com/in/arjun-kris" target="_blank">Arjun
                                        Krishna*</a>,
                                    </nobr>

                                    <nobr><a href="https://core-robotics.gatech.edu/people/zulfiqar-zaidi/"
                                             target="_blank">Zulfiqar Zaidi</a>,
                                    </nobr>

                                    <nobr><em>Rohan Paleja</em>,</nobr>

                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Letian Chen</a>,</nobr>


                                    <nobr><a href="https://www.linkedin.com/in/erin-hedlund-botti" target="_blank">Erin
                                        Hedlund-Botti</a>,
                                    </nobr>

                                    <nobr><a href="https://core-robotics.gatech.edu/people/mariah-schrum/"
                                             target="_blank">Mariah Schrum</a>,
                                    </nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        ACM/IEEE International Conference on Human-Robot Interaction (HRI), 2023
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#krishna2023HRI-abstract" role="button" aria-expanded="false"
                                       aria-controls="krishna2023HRI-abstract">Abstract</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/krishna_VAIN.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/CORE-Robotics-Lab/ICCT" target="_blank">Code</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="krishna2023HRI-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            As high-speed, agile robots become more commonplace, these robots will have
                                            the potential to better aid and collaborate with humans. However, due to the
                                            increased agility and functionality of these robots, close collaboration
                                            with humans can create safety concerns that alter team dynamics and degrade
                                            task performance. In this work, we aim to enable the deployment of safe and
                                            trustworthy agile robots that operate in proximity with humans. We do so by
                                            1) Proposing a novel human-robot doubles table tennis scenario to serve as a
                                            testbed for studying agile, proximate human-robot collaboration and 2)
                                            Conducting a user-study to understand how attributes of the robot (e.g.,
                                            robot competency or capacity to communicate) impact team dynamics, perceived
                                            safety, and perceived trust, and how these latent factors affect human-robot
                                            collaboration (HRC) performance. We find that robot competency significantly
                                            increases perceived trust ($p<.001$), extending skill-to-trust assessments
                                            in prior studies to agile, proximate HRC. Furthermore, interestingly, we
                                            find that when the robot vocalizes its intention to perform a task, it
                                            results in a significant decrease in team performance (p=.037) and perceived
                                            safety of the system (p=.009).
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>
            </ol>

            </ol>
        </div>
    </div>

    <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
        <div class="col-sm-1 mt-2 p-0 pr-1">
            <h3 class="bibliography-year">2022</h3>
        </div>
        <div class="col-sm-11 p-0">
            <ol class="bibliography">

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://www.robot-learning.org/" target="_blank">
                                CoRL
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="chen2022CoRL" class="col p-0">
                                <h5 class="title mb-0">Fast Lifelong Adaptive Inverse Reinforcement Learning from
                                    Demonstrations</h5>
                                <div class="author">

                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Letian Chen*</a>,</nobr>

                                    <nobr><a href="https://www.linkedin.com/in/sravan-jayanthi" target="_blank">Sravan
                                        Jayanthi*</a>,
                                    </nobr>

                                    <nobr><em>Rohan Paleja</em>,</nobr>

                                    <nobr><a href="https://www.linkedin.com/in/danielmartin576" target="_blank">Daniel
                                        Martin</a>,
                                    </nobr>


                                    <nobr><a href="https://www.linkedin.com/in/stevezakharov" target="_blank">Viacheslav
                                        Zakharov</a>,
                                    </nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Conference of Robot Learning, 2022
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#chen2022CoRL-abstract" role="button" aria-expanded="false"
                                       aria-controls="chen2022CoRL-abstract">Abstract</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/chen_flair.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/CORE-Robotics-Lab/ICCT" target="_blank">Code</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="chen2022CoRL-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Learning from Demonstration (LfD) approaches empower end-users to teach
                                            robots novel tasks via demonstrations of the desired behaviors,
                                            democratizing access to robotics. However, current LfD frameworks are not
                                            capable of fast adaptation to heterogeneous human demonstrations nor the
                                            large-scale deployment in ubiquitous robotics applications. In this paper,
                                            we propose a novel LfD framework, Fast Lifelong Adaptive Inverse
                                            Reinforcement learning (FLAIR). Our approach (1) leverages learned
                                            strategies to construct policy mixtures for fast adaptation to new
                                            demonstrations, allowing for quick end-user personalization, (2) distills
                                            common knowledge across demonstrations, achieving accurate task inference;
                                            and (3) expands its model only when needed in lifelong deployments,
                                            maintaining a concise set of prototypical strategies that can approximate
                                            all behaviors via policy mixtures. We empirically validate that FLAIR
                                            achieves adaptability (i.e., the robot adapts to heterogeneous,
                                            user-specific task preferences), efficiency (i.e., the robot achieves
                                            sample-efficient adaptation), and scalability (i.e., the model grows
                                            sublinearly with the number of demonstrations while maintaining high
                                            performance). FLAIR surpasses benchmarks across three control tasks with an
                                            average 57% improvement in policy returns and an average 78% fewer episodes
                                            required for demonstration modeling using policy mixtures. Finally, we
                                            demonstrate the success of FLAIR in a table tennis task and find users rate
                                            FLAIR as having higher task (p < .05) and personalization (p < .05)
                                            performance.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>


                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold pink darken-1 align-middle" style="width: 65px;"
                               href="https://www.agilerobotscorl2022.com/" target="_blank">
                                CoRL
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="krishna2022Agility" class="col p-0">
                                <h5 class="title mb-0">Utilizing Human Feedback for Primitive Optimization in Wheelchair
                                    Tennis</h5>
                                <div class="author">
                                    <nobr><a href="https://www.linkedin.com/in/arjun-kris" target="_blank">Arjun
                                        Krishna</a>,
                                    </nobr>

                                    <nobr><a href="https://core-robotics.gatech.edu/people/zulfiqar-zaidi/"
                                             target="_blank">Zulfiqar Zaidi</a>,
                                    </nobr>


                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Letian Chen</a>,</nobr>

                                    <nobr><em>Rohan Paleja</em>,</nobr>

                                    <nobr><a href="https://esmaeilseraj09.wixsite.com/home" target="_blank">Esmaeil
                                        Seraj</a>,
                                    </nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        CoRL 2022 Learning for Agile Robotics Workshop
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#krishna2022Agility-abstract" role="button" aria-expanded="false"
                                       aria-controls="krishna2022Agility-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://sites.gatech.edu/core-robotics/files/2022/12/ProMP_WTR_final_draft.pdf"
                                       target="_blank">PDF</a>

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="krishna2022Agility-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Agile robotics presents a difficult challenge with robots moving at high
                                            speeds requiring precise and low-latency sensing and control. Creating agile
                                            motion that accomplishes the task at hand while being safe to execute is a
                                            key requirement for agile robots to gain human trust. This requires
                                            designing new approaches that are flexible and maintain knowledge over world
                                            constraints. In this paper, we consider the problem of building a flexible
                                            and adaptive controller for a challenging agile mobile manipulation task of
                                            hitting ground strokes on a wheelchair tennis robot. We propose and evaluate
                                            an extension to the work done on learning striking behaviors using a
                                            probabilistic movement primitive (ProMP) framework by (1) demonstrating the
                                            safe execution of learned primitives on an agile mobile manipulator setup,
                                            and (2) proposing an online primitive refinement procedure that utilizes
                                            evaluative feedback from humans on the executed trajectories.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>

                </li>

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://roboticsconference.org/" target="_blank">
                                RSS
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="paleja2022RSS" class="col p-0">
                                <h5 class="title mb-0">Learning Interpretable, High-Performing Policies for Autonomous
                                    Driving</h5>
                                <div class="author">


                                    <nobr><em>Rohan Paleja*</em>,</nobr>

                                    <nobr><a href="https://yaruniu.com/" target="_blank">Yaru Niu
                                        <nobr><em>*</em></nobr>
                                    </a>,
                                    </nobr>

                                    <nobr><a href="https://www.andrew-silva.com/" target="_blank">Andrew Silva</a>,
                                    </nobr>

                                    <nobr><a href="https://www.linkedin.com/in/chace-ritchie" target="_blank">Chace
                                        Ritchie</a>,
                                    </nobr>

                                    <nobr><a href="https://www.linkedin.com/in/sugju-choi" target="_blank">Sugju
                                        Choi</a>,
                                    </nobr>

                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Robotics: Science and Systems, 2022
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#paleja2022RSS-abstract" role="button" aria-expanded="false"
                                       aria-controls="paleja2022RSS-abstract">Abstract</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/ICCT/2202.02352.pdf" target="_blank">PDF</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://github.com/CORE-Robotics-Lab/ICCT" target="_blank">Code</a>

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="paleja2022RSS-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Gradient-based approaches in reinforcement learning have achieved tremendous
                                            success in learning policies for autonomous vehicles. While the performance
                                            of these approaches warrants real-world adoption, these policies lack
                                            interpretability, limiting deployability in the safety-critical and
                                            legally-regulated domain of autonomous driving (AD). AD requires
                                            interpretable and verifiable control policies that maintain high
                                            performance. We propose Interpretable Continuous Control Trees (ICCTs), a
                                            tree-based model that can be optimized via modern, gradient-based, RL
                                            approaches to produce high-performing, interpretable policies. The key to
                                            our approach is a procedure for allowing direct optimization in a sparse
                                            decision-tree-like representation. We validate ICCTs against baselines
                                            across six domains, showing that ICCTs are capable of learning interpretable
                                            policy representations that parity or outperform baselines by up to 33% in
                                            AD scenarios while achieving a 300x-600x reduction in the number of policy
                                            parameters against deep learning baselines. Furthermore, we demonstrate the
                                            interpretability and utility of our ICCTs through a 14-car physical robot
                                            demonstration.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>


                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold pink darken-1 align-middle" style="width: 65px;"
                               href="https://sites.google.com/view/rss22-srl/home" target="_blank">
                                RSS
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="pimentel2022Scaling" class="col p-0">
                                <h5 class="title mb-0">Scaling Multi-Agent Reinforcement Learning via State
                                    Upsampling</h5>
                                <div class="author">
                                    <nobr><a href="https://www.luismpimentel.com/" target="_blank">Luis Pimentel
                                        <nobr><em>*</em></nobr>
                                    </a>,
                                    </nobr>
                                    <nobr><em>Rohan Paleja*</em>,</nobr>

                                    <nobr><a href="https://phejohnwang.github.io/" target="_blank">Zheyuan Wang
                                        <nobr></nobr>
                                    </a>,
                                    </nobr>

                                    <nobr><a href="https://esmaeilseraj09.wixsite.com/home" target="_blank">Esmaeil
                                        Seraj</a>,
                                    </nobr>
                                    <nobr><a href="https://www.linkedin.com/in/james-pagan" target="_blank">James
                                        Pagan</a>,
                                    </nobr>

                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        RSS 2022 Workshop on Scaling Robot Learning (RSS22-SRL)
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#pimentel2022Scaling-abstract" role="button" aria-expanded="false"
                                       aria-controls="pimentel2022Scaling-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/RSS22_SRL_Workshop_Paper_final_accepted.pdf"
                                       target="_blank">PDF</a>

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="pimentel2022Scaling-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            We consider the problem of scaling Multi-Agent Reinforcement Learning (MARL)
                                            algorithms toward larger environments and team sizes. While it is possible
                                            to learn a MARL-synthesized policy on these larger problems from scratch,
                                            training is difficult as the joint state-action space is much larger. Policy
                                            learning will require a large amount of experience (and associated training
                                            time) to reach a target performance. In this paper, we propose a transfer
                                            learning method that accelerates the training performance in such
                                            high-dimensional tasks with increased complexity. Our method upsamples an
                                            agent’s state representation in a smaller, less challenging, source task in
                                            order to pre-train a target policy for a larger, more challenging, target
                                            task. By transferring the policy after pre-training and continuing MARL in
                                            the target domain, the information learned within the source task enables
                                            higher performance within the target task in significantly less time than
                                            training from scratch. As such, our method enables the scalability of
                                            coordination problems. Furthermore, as our method only changes the state
                                            representation of agents across tasks, it is agnostic to the policy’s
                                            architecture and can be deployed across different MARL algorithms. We
                                            provide results showing that a policy trained under our method is able to
                                            achieve up to a 7.88$\times$ performance improvement under the same amount
                                            of training time, compared to a policy trained from scratch. Moreover, our
                                            method enables learning in difficult target task settings where training
                                            from scratch fails.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>

                </li>

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://aamas2022-conference.auckland.ac.nz/" target="_blank">
                                AAMAS
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="paleja2022AAMAS" class="col p-0">
                                <h5 class="title mb-0">Learning Efficient Diverse Communication for Cooperative
                                    Heterogeneous Teaming </h5>
                                <div class="author">

                                    <nobr><a href="https://esmaeilseraj09.wixsite.com/home" target="_blank">Esmaeil
                                        Seraj
                                        <nobr><em>*</em></nobr>
                                    </a>,
                                    </nobr>

                                    <nobr><a href="https://phejohnwang.github.io/" target="_blank">Zheyuan Wang
                                        <nobr><em>*</em></nobr>
                                    </a>,
                                    </nobr>

                                    <nobr><em>Rohan Paleja*</em>,</nobr>

                                    <nobr><a href="https://www.linkedin.com/in/danielmartin576" target="_blank">Daniel
                                        Martin</a>,
                                    </nobr>

                                    <nobr><a href="https://www.linkedin.com/in/matthew-sklar-422231b4" target="_blank">Matthew
                                        Sklar</a>,
                                    </nobr>

                                    <nobr><a href="https://www.linkedin.com/in/anirudh-patel-7124a1b8" target="_blank">Anirudh
                                        Patel</a>,
                                    </nobr>

                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        International Conference on Autonomous Agents and Multiagent Systems (AAMAS),
                                        2022
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#paleja2022AAMAS-abstract" role="button" aria-expanded="false"
                                       aria-controls="paleja2022AAMAS-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/paleja_hetnet.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/nvidia.com/active-concept-learning" target="_blank">Project Website</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="paleja2022AAMAS-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            High-performing teams learn intelligent and efficient communication and
                                            coordination strategies to maximize their joint utility. These teams
                                            implicitly understand the different roles of heterogeneous team members and
                                            adapt their communication protocols accordingly. Multi-Agent Reinforcement
                                            Learning (MARL) seeks to develop computational methods for synthesizing such
                                            coordination strategies, but formulating models for heterogeneous teams with
                                            different state, action, and observation spaces has remained an open
                                            problem. Without properly modeling agent heterogeneity, as in prior MARL
                                            work that leverages homogeneous graph networks, communication becomes less
                                            helpful and can even deteriorate the cooperativity and team performance. We
                                            propose Heterogeneous Policy Networks (HetNet) to learn efficient and
                                            diverse communication models for coordinating cooperative heterogeneous
                                            teams. Building on heterogeneous graph-attention networks, we show that
                                            HetNet not only facilitates learning heterogeneous collaborative policies
                                            per existing agent-class but also enables end-to-end training for learning
                                            highly efficient binarized messaging.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>


                </li>

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold pink darken-1 align-middle" style="width: 65px;"
                               href="https://aamas2022-conference.auckland.ac.nz/" target="_blank">
                                AAAI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="paleja2022AAAI" class="col p-0">
                                <h5 class="title mb-0">Mutual Understanding in Human-Machine Teaming</h5>
                                <div class="author">

                                    <nobr><em>Rohan Paleja*</em>,</nobr>

                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Association for the Advancement of Artificial Intelligence Conference (AAAI)
                                        Doctoral Consortium, 2022
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#paleja2022AAAI-abstract" role="button" aria-expanded="false"
                                       aria-controls="paleja2022AAAI-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/AAAI_DC_Rohan_Paleja.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/nvidia.com/active-concept-learning" target="_blank">Project Website</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="paleja2022AAAI-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Collaborative robots (i.e., “cobots”) and machine learning-based virtual
                                            agents are increasingly entering the human workspace with the aim of
                                            increasing productivity, enhancing safety, and improving the quality of our
                                            lives. These agents will dynamically interact with a wide variety of people
                                            in dynamic and novel contexts, increasing the prevalence of human-machine
                                            teams in healthcare, manufacturing, and search-and-rescue. In this research,
                                            we enhance the mutual understanding within a human-machine team by enabling
                                            cobots to understand heterogeneous teammates via person-specific embeddings,
                                            identifying contexts in which xAI methods can help improve team mental model
                                            alignment, and enabling cobots to effectively communicate information that
                                            supports high-performance human-machine teaming.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>

            </ol>
        </div>
    </div>


    <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
        <div class="col-sm-1 mt-2 p-0 pr-1">
            <h3 class="bibliography-year">2021</h3>
        </div>
        <div class="col-sm-11 p-0">
            <ol class="bibliography">


                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold orange darken-1 align-middle" style="width: 65px;"
                               href="https://www.tandfonline.com/journals/tciv20" target="_blank">
                                CMBBE
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="Dias2021" class="col p-0">
                                <h5 class="title mb-0">Using Machine Learning to Predict Perfusionists Critical
                                    Decision-Making during Cardiac Surgery</h5>
                                <div class="author">

                                    <nobr>Roger Dias
                                        <nobr><em></em></nobr>
                                        </a>,
                                    </nobr>

                                    <nobr>Marco Zenati,</nobr>
                                    <nobr>Geoff Rance,</nobr>
                                    <nobr>Rithy Srey,</nobr>
                                    <nobr>David Arney,</nobr>
                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Letian Chen</a>,</nobr>
                                    <nobr><em>Rohan Paleja</em>,</nobr>

                                    <nobr><a href="https://scholar.harvard.edu/laurenkennedy-metz/home" target="_blank">Lauren
                                        Kennedy-Metz</a>,
                                    </nobr>

                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Computer Methods in Biomechanics and Biomedical Engineering, 2021
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#Dias2021-abstract" role="button" aria-expanded="false"
                                       aria-controls="Dias2021-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://www.tandfonline.com/doi/abs/10.1080/21681163.2021.2002724?scroll=top&needAccess=true&journalCode=tciv20"
                                       target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/A4k3B3uewBs?t=8337" target="_blank">Talk</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="Dias2021-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            The cardiac surgery operating room is a high-risk and complex environment in
                                            which multiple experts work as a team to provide safe and excellent care to
                                            patients. During the cardiopulmonary bypass phase of cardiac surgery,
                                            critical decisions need to be made and the perfusionists play a crucial role
                                            in assessing available information and taking a certain course of action. In
                                            this paper, we report the findings of a simulation-based study using machine
                                            learning to build predictive models of perfusionists’ decision-making during
                                            critical situations in the operating room (OR). Performing 30-fold
                                            cross-validation across 30 random seeds, our machine learning approach was
                                            able to achieve an accuracy of 78.2% (95% confidence interval: 77.8% to
                                            78.6%) in predicting perfusionists’ actions, having access to only 148
                                            simulations. The findings from this study may inform future development of
                                            computerised clinical decision support tools to be embedded into the OR,
                                            improving patient safety and surgical outcomes.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>


                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://neurips.cc/Conferences/2021/" target="_blank">
                                NeurIPS
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="Paleja2021NeurIPS" class="col p-0">
                                <h5 class="title mb-0">The Utility of Explainable AI in Ad Hoc Human-Machine
                                    Teaming</h5>
                                <div class="author">

                                    <nobr><em>Rohan Paleja</em>,</nobr>

                                    <nobr><a href="https://www.linkedin.com/in/muyleng-ghuy" target="_blank">Muyleng
                                        Ghuy</a>,
                                    </nobr>
                                    <nobr><a href="https://www.linkedin.com/in/nadun-ranawaka-arachchige-87701b137"
                                             target="_blank">Nadun Ranawaka Arachchige</a>,
                                    </nobr>
                                    <nobr><a href="https://ilp.mit.edu/node/44597" target="_blank">Reed Jensen</a>,
                                    </nobr>
                                    and
                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Conference on Neural Information Processing Systems (NeurIPS), 2021
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#Paleja2021NeurIPS-abstract" role="button" aria-expanded="false"
                                       aria-controls="Paleja2021NeurIPS-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/the_utility_of_explainable_ai_.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arjunsripathy.github.io/model_switching/" target="_blank">Project Website</a>-->
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/arjunsripathy/model_switching" target="_blank">Code</a>-->
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://papertalk.org/papertalks/37000" target="_blank">Talk</a>

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="Paleja2021NeurIPS-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Recent advances in machine learning have led to growing interest in
                                            Explainable AI (xAI) to enable humans to gain insight into the
                                            decision-making of machine learning models. Despite this recent interest,
                                            the utility of xAI techniques has not yet been characterized in
                                            human-machine teaming. Importantly, xAI offers the promise of enhancing team
                                            situational awareness (SA) and shared mental model development, which are
                                            the key characteristics of effective human-machine teams. Rapidly developing
                                            such mental models is especially critical in ad hoc human-machine teaming,
                                            where agents do not have a priori knowledge of others’ decision-making
                                            strategies. In this paper, we present two novel human-subject experiments
                                            quantifying the benefits of deploying xAI techniques within a human-machine
                                            teaming scenario. First, we show that xAI techniques can support SA
                                            ($p<0.05)$. Second, we examine how different SA levels induced via a
                                            collaborative AI policy abstraction affect ad hoc human-machine teaming
                                            performance. Importantly, we find that the benefits of xAI are not
                                            universal, as there is a strong dependence on the composition of the
                                            human-machine team. Novices benefit from xAI providing increased SA
                                            ($p<0.05$) but are susceptible to cognitive overhead ($p<0.05$). On the
                                            other hand, expert performance degrades with the addition of xAI-based
                                            support ($p<0.05$), indicating that the cost of paying attention to the xAI
                                            outweighs the benefits obtained from being provided additional information
                                            to enhance SA. Our results demonstrate that researchers must deliberately
                                            design and deploy the right xAI techniques in the right scenario by
                                            carefully considering human-machine team composition and how the xAI method
                                            augments SA.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>


                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold pink darken-1 align-middle" style="width: 65px;"
                               href="https://ai-hri.github.io/2021/" target="_blank">
                                AI-HRI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="chen2021AIHRI" class="col p-0">
                                <h5 class="title mb-0">Towards Sample-efficient Apprenticeship Learning from Suboptimal
                                    Demonstration</h5>
                                <div class="author">
                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Letian Chen</a>,</nobr>

                                    <nobr><em>Rohan Paleja</em>,</nobr>

                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>


                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        AAAI Artificial Intelligence for Human-Robot Interaction (AI-HRI) Fall
                                        Symposium, 2021
                                        <br>
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#chen2021AIHRI-abstract" role="button" aria-expanded="false"
                                       aria-controls="chen2021AIHRI-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/SSRR_Extension_AI_HRI-final.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/view/feature-learning" target="_blank">Project Website</a>-->
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/andreea7b/FERL" target="_blank">Code</a>-->
                                    <!--	  <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=NaD-eVri8r4&ab_channel=InterACTLab" target="_blank">Talk</a>-->

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="chen2021AIHRI-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Learning from Demonstration (LfD) seeks to democratize robotics by enabling
                                            non-roboticist end-users to teach robots to perform novel tasks by providing
                                            demonstrations. However, as demonstrators are typically non-experts, modern
                                            LfD techniques are unable to produce policies much better than the
                                            suboptimal demonstration. A previously-proposed framework, SSRR, has shown
                                            success in learning from suboptimal demonstration but relies on
                                            noise-injected trajectories to infer an idealized reward function. A random
                                            approach such as noise-injection to generate trajectories has two key
                                            drawbacks: 1) Performance degradation could be random depending on whether
                                            the noise is applied to vital states and 2) Noise-injection generated
                                            trajectories may have limited suboptimality and therefore will not
                                            accurately represent the whole scope of suboptimality. We present Systematic
                                            Self-Supervised Reward Regression, S3RR, to investigate systematic
                                            alternatives for trajectory degradation.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>


                </li>

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://aamas2021.soton.ac.uk/" target="_blank">
                                AAMAS
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="paleja2021AAMAS" class="col p-0">
                                <h5 class="title mb-0">Multi-Agent Graph-Attention Communication and Teaming</h5>
                                <div class="author">
                                    <nobr><a href="https://yaruniu.com/" target="_blank">Yaru Niu
                                        <nobr><em>*</em></nobr>
                                    </a>,
                                    </nobr>

                                    <nobr><em>Rohan Paleja*</em>,</nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        International Conference on Autonomous Agents and Multiagent Systems (AAMAS),
                                        2021
                                        <br>
                                        <nobr><em style="color:orange;">Best Workshop Paper Award Winner at ICCV MAIR2
                                            Workshop</em></nobr>
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#paleja2021AAMAS-abstract" role="button" aria-expanded="false"
                                       aria-controls="paleja2021AAMAS-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p964.pdf"
                                       target="_blank">PDF</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://github.com/CORE-Robotics-Lab/MAGIC" target="_blank">Code</a>
                                    <br>
                                    <iframe src="https://ghbtns.com/github-btn.html?user=CORE-Robotics-Lab&repo=MAGIC&type=star&count=true"
                                            frameborder="0" scrolling="0" width="100" height="27" title="GitHub Star"
                                            style="padding-top: 5px;"></iframe>
                                    <br>
                                    <iframe src="https://ghbtns.com/github-btn.html?user=CORE-Robotics-Lab&repo=MAGIC&type=fork&count=true"
                                            frameborder="0" scrolling="0" width="100" height="20" title="GitHub Fork"
                                            style="padding-top: 0px;"></iframe>

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="paleja2021AAMAS-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            High-performing teams learn effective communication strategies to
                                            judiciously share information and reduce the cost of communication overhead.
                                            Within multi-agent reinforcement learning, synthesizing effective policies
                                            requires reasoning about when to communicate, whom to communicate with, and
                                            how to process messages. We propose a novel multi-agent reinforcement
                                            learning algorithm, Multi-Agent Graph-attentIon Communication (MAGIC), with
                                            a graph-attention communication protocol in which we learn 1) a Scheduler to
                                            help with the problems of when to communicate and whom to address messages
                                            to, and 2) a Message Processor using Graph Attention Networks (GATs) with
                                            dynamic graphs to deal with communication signals. The Scheduler consists of
                                            a graph attention encoder and a differentiable attention mechanism, which
                                            outputs dynamic, differentiable graphs to the Message Processor, which
                                            enables the Scheduler and Message Processor to be trained end-to-end. We
                                            evaluate our approach on a variety of cooperative tasks, including Google
                                            Research Football. Our method outperforms baselines across all domains,
                                            achieving $\approx 10\%$ increase in reward in the most challenging domain.
                                            We also show MAGIC communicates $23.2\%$ more efficiently than the average
                                            baseline, is robust to stochasticity, and scales to larger state-action
                                            spaces. Finally, we demonstrate MAGIC on a physical, multi-robot testbed.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>


                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://humanrobotinteraction.org/2021/" target="_blank">
                                HRI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="schrum2021HRI" class="col p-0">
                                <h5 class="title mb-0"> Effects of Social Factors and Team Dynamics on Adoption of
                                    Collaborative Robot Autonomy</h5>
                                <div class="author">

                                    <nobr>Mariah Schrum*,</nobr>
                                    <nobr>Glen Neville*,</nobr>
                                    <nobr>Michael Johnson*,</nobr>
                                    <nobr>Nina Moorman,</nobr>
                                    <nobr><em>Rohan Paleja</em>,</nobr>
                                    <nobr>Karen Feigh,</nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        ACM/IEEE International Conference on Human Robot Interaction (HRI), 2021
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#schrum2021HRI-abstract" role="button" aria-expanded="false"
                                       aria-controls="schrum2021HRI-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://core-robotics.gatech.edu/files/2021/03/HumanTrust-Roman19.pdf"
                                       target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/view/less-human-decision-model" target="_blank">Project Website</a>-->
                                    <!--	        <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=xa_l5HeyVgw" target="_blank">Talk</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="schrum2021HRI-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            As automation becomes more prevalent, the fear of job loss due to automation
                                            increases. Workers may not be amenable to working with a robotic co-worker
                                            due to a negative perception of the technology. The attitudes of workers
                                            towards automation are influenced by a variety of complex and multi-faceted
                                            factors such as intention to use, perceived usefulness and other external
                                            variables. In an analog manufacturing environment, we explore how these
                                            various factors influence an individual’s willingness to work with a robot
                                            over a human co-worker in a collaborative Lego building task. We
                                            specifically explore how this willingness is affected by: 1) the level of
                                            social rapport established between the individual and his or her human
                                            co-worker, 2) the anthropomorphic qualities of the robot, and 3) factors
                                            including trust, fluency and personality traits. Our results show that a
                                            participant’s willingness to work with automation decreased due to lower
                                            perceived team fluency (p=0.045), rapport established between a participant
                                            and their co-worker (p=0.003), the gender of the participant being male
                                            (p=0.041), and a higher inherent trust in people (p=0.018).
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>
            </ol>


            </ol>
        </div>
    </div>

    <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
        <div class="col-sm-1 mt-2 p-0 pr-1">
            <h3 class="bibliography-year">2020</h3>
        </div>
        <div class="col-sm-11 p-0">
            <ol class="bibliography">
                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://www.robot-learning.org/" target="_blank">
                                CoRL
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="chen2020CoRL" class="col p-0">
                                <h5 class="title mb-0">Learning from Suboptimal Demonstration via Self-Supervised Reward
                                    Regression</h5>
                                <div class="author">

                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Letian Chen</a>,</nobr>
                                    <nobr><em>Rohan Paleja</em>,</nobr>
                                    and
                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Conference on Robot Learning (CoRL), 2021
                                        <br>
                                        <nobr><em style="color:orange;">Best Paper Award Finalist</em></nobr>
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#chen2020CoRL-abstract" role="button" aria-expanded="false"
                                       aria-controls="chen2020CoRL-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/chen_self_supervised.pdf" target="_blank">PDF</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://github.com/CORE-Robotics-Lab/SSRR" target="_blank">Code</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://www.youtube.com/watch?v=tjGEoe33HAg" target="_blank">Talk</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://www.youtube.com/watch?v=4TYfeymwj3E" target="_blank">Spotlight
                                        Talk</a><br>
                                    <iframe src="https://ghbtns.com/github-btn.html?user=CORE-Robotics-Lab&repo=SSRR&type=star&count=true"
                                            frameborder="0" scrolling="0" width="100" height="27" title="GitHub"
                                            style="padding-top: 5px;"></iframe>
                                    <br>
                                    <iframe src="https://ghbtns.com/github-btn.html?user=CORE-Robotics-Lab&repo=SSRR&type=fork&count=true"
                                            frameborder="0" scrolling="0" width="100" height="20"
                                            title="GitHub"></iframe>
                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="chen2020CoRL-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Learning from Demonstration (LfD) seeks to democratize robotics by enabling
                                            non-roboticist end-users to teach robots to perform a task by providing a
                                            human demonstration. However, modern LfD techniques, e.g. inverse
                                            reinforcement learning (IRL), assume users provide at least stochastically
                                            optimal demonstrations. This assumption fails to hold in most real-world
                                            scenarios. Recent attempts to learn from sub-optimal demonstration leverage
                                            pairwise rankings and following the Luce-Shepard rule. However, we show
                                            these approaches make incorrect assumptions and thus suffer from brittle,
                                            degraded performance. We overcome these limitations in developing a novel
                                            approach that bootstraps off suboptimal demonstrations to synthesize
                                            optimality-parameterized data to train an idealized reward function. We
                                            empirically validate we learn an idealized reward function with ~0.95
                                            correlation with ground-truth reward versus ~0.75 for prior work. We can
                                            then train policies achieving ~200% improvement over the suboptimal
                                            demonstration and ~90% improvement over prior work. We present a physical
                                            demonstration of teaching a robot a topspin strike in table tennis that
                                            achieves 32% faster returns and 40% more topspin than user demonstration.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://neurips.cc" target="_blank">
                                NeurIPS
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="paleja2020NeurIPS" class="col p-0">
                                <h5 class="title mb-0">Interpretable and Personalized Apprenticeship Scheduling:
                                    Learning Interpretable Scheduling Policies from Heterogeneous User
                                    Demonstrations </h5>
                                <div class="author">

                                    <nobr><em>Rohan Paleja</em>,</nobr>
                                    <nobr><a href="https://www.andrew-silva.com/" target="_blank">Andrew Silva</a>,
                                    </nobr>
                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Letian Chen</a>,</nobr>
                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>


                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Conference on Neural Information Processing Systems (NeurIPS), 2020.
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#paleja2020NeurIPS-abstract" role="button" aria-expanded="false"
                                       aria-controls="paleja2020NeurIPS-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/paleja_pnts.pdf" target="_blank">PDF</a>

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="paleja2020NeurIPS-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Resource scheduling and coordination is an NP-hard optimization requiring an
                                            efficient allocation of agents to a set of tasks with upper- and lower bound
                                            temporal and resource constraints. Due to the large-scale and dynamic nature
                                            of resource coordination in hospitals and factories, human domain experts
                                            manually plan and adjust schedules on the fly. To perform this job, domain
                                            experts leverage heterogeneous strategies and rules-of-thumb honed over
                                            years of apprenticeship. What is critically needed is the ability to extract
                                            this domain knowledge in a heterogeneous and interpretable apprenticeship
                                            learning framework to scale beyond the power of a single human expert, a
                                            necessity in safety-critical domains. We propose a personalized and
                                            interpretable apprenticeship scheduling algorithm that infers an
                                            interpretable representation of all human task demonstrators by extracting
                                            decision-making criteria specified by an inferred, personalized embedding
                                            without constraining the number of decision-making strategies. We achieve
                                            near-perfect LfD accuracy in synthetic domains and 88.22% accuracy on a
                                            real-world planning domain, outperforming baselines. Further, a user study
                                            conducted shows that our methodology produces both interpretable and highly
                                            usable models (p < 0.05).
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>


                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://humanrobotinteraction.org/2020/" target="_blank">
                                HRI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="chen2020HRI" class="col p-0">
                                <h5 class="title mb-0">Joint Goal and Strategy Inference across Heterogeneous
                                    Demonstrators via Reward Network Distillation </h5>
                                <div class="author">

                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Letian Chen</a>,</nobr>
                                    <nobr><em>Rohan Paleja</em>,</nobr>
                                    <nobr><a href="https://www.linkedin.com/in/muyleng-ghuy" target="_blank">Muyleng
                                        Ghuy</a>,
                                    </nobr>

                                    and
                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        ACM/IEEE International Conference on Human Robot Interaction (HRI), 2020.
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#chen2020HRI-abstract" role="button" aria-expanded="false"
                                       aria-controls="chen2020HRI-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/chen_msrd.pdf" target="_blank">PDF</a>

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="chen2020HRI-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Reinforcement learning (RL) has achieved tremendous success as a general
                                            framework for learning how to make decisions. However, this success relies
                                            on the interactive hand-tuning of a reward function by RL experts. On the
                                            other hand, inverse reinforcement learning (IRL) seeks to learn a reward
                                            function from readily-obtained human demonstrations. Yet, IRL suffers from
                                            two major limitations: 1)reward ambiguity – there are an infinite number of
                                            possible re-ward functions that could explain an expert’s demonstration and
                                            2) heterogeneity-human experts adopt varying strategies and preferences,
                                            which makes learning from multiple demonstrators difficult due to the common
                                            assumption that demonstrators seeks to maximize the same reward. In this
                                            work, we propose a method to jointly infer a task goal and humans’ strategic
                                            preferences via network distillation. This approach enables us to distill a
                                            robust task reward (addressing reward ambiguity) and to model each
                                            strategy’s objective (handling heterogeneity). We demonstrate our algorithm
                                            can better recover task reward and strategy rewards and imitate the
                                            strategies two simulated tasks and a real-world table tennis task.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>

            </ol>

            </ol>
        </div>
    </div>

    <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
        <div class="col-sm-1 mt-2 p-0 pr-1">
            <h3 class="bibliography-year">2019</h3>
        </div>
        <div class="col-sm-11 p-0">
            <ol class="bibliography">
                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold pink darken-1 align-middle" style="width: 65px;"
                               href="https://hripioneers.org/" target="_blank">
                                HRI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="paleja2019HRIPioneers" class="col p-0">
                                <h5 class="title mb-0">Heterogeneous Learning from Demonstration</h5>
                                <div class="author">

                                    <nobr><em>Rohan Paleja</em>,</nobr>

                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        International Conference on Human Robot Interaction (HRI) Pioneers Workshop
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#paleja2019HRIPioneers-abstract" role="button" aria-expanded="false"
                                       aria-controls="paleja2019HRIPioneers-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://core-robotics.gatech.edu/wp-content/uploads/sites/958/2019/01/Paleja-Heterogeneous-Learning-from-Demonstration.pdf"
                                       target="_blank">PDF</a>

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="paleja2019HRIPioneers-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            The development of human-robot systems able to leverage the strengths of
                                            both humans and their robotic counterparts has been greatly sought after
                                            because of the foreseen, broad-ranging impact across industry and research.
                                            We believe the true potential of these systems cannot be reached unless the
                                            robot is able to act with a high level of autonomy, reducing the burden of
                                            manual tasking or teleoperation. To achieve this level of autonomy, robots
                                            must be able to work fluidly with its human partners, inferring their needs
                                            without explicit commands. This inference requires the robot to be able to
                                            detect and classify the heterogeneity of its partners. We propose a
                                            framework for learning from heterogeneous demonstration based upon Bayesian
                                            inference and evaluate a suite of approaches on a real-world dataset of
                                            gameplay from StarCraft II. This evaluation provides evidence that our
                                            Bayesian approach can outperform conventional methods by up to 12.8%.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>
            </ol>
        </div>
    </div>
</div>

<!-- Footer -->
<footer>
    &copy; Copyright 2023 Rohan Paleja.


</footer>
<!-- ShowPubs choices -->
<script>showPubs(0);</script>
<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>


<!-- Core JavaScript Files -->
<script src="/assets/js/jquery.min.js" type="text/javascript"></script>
<script src="/assets/js/popper.min.js" type="text/javascript"></script>
<script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
<script src="/assets/js/mdb.min.js" type="text/javascript"></script>
<script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js"
        integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D"
        crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
<script src="/assets/js/common.js"></script>

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
    $(document).ready(function () {
        var navbarHeight = $('#navbar').outerHeight(true);
        $('body').css({'padding-top': navbarHeight});
        $('progress-container').css({'padding-top': navbarHeight});
        var progressBar = $('#progress');
        progressBar.css({'top': navbarHeight});
        var getMax = function () {
            return $(document).height() - $(window).height();
        }
        var getValue = function () {
            return $(window).scrollTop();
        }
        // Check if the browser supports the progress element.
        if ('max' in document.createElement('progress')) {
            // Set the 'max' attribute for the first time.
            progressBar.attr({max: getMax()});
            progressBar.attr({value: getValue()});

            $(document).on('scroll', function () {
                // On scroll only the 'value' attribute needs to be calculated.
                progressBar.attr({value: getValue()});
            });

            $(window).resize(function () {
                var navbarHeight = $('#navbar').outerHeight(true);
                $('body').css({'padding-top': navbarHeight});
                $('progress-container').css({'padding-top': navbarHeight});
                progressBar.css({'top': navbarHeight});
                // On resize, both the 'max' and 'value' attributes need to be calculated.
                progressBar.attr({max: getMax(), value: getValue()});
            });
        } else {
            var max = getMax(), value, width;
            var getWidth = function () {
                // Calculate the window width as a percentage.
                value = getValue();
                width = (value / max) * 100;
                width = width + '%';
                return width;
            }
            var setWidth = function () {
                progressBar.css({width: getWidth()});
            };
            setWidth();
            $(document).on('scroll', setWidth);
            $(window).on('resize', function () {
                // Need to reset the 'max' attribute.
                max = getMax();
                setWidth();
            });
        }
    });


</script>

<!-- Code Syntax Highlighting -->
<link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
<script src="/assets/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Script Used for Randomizing the Projects Order -->
<!-- <script type="text/javascript">
  $.fn.shuffleChildren = function() {
    $.each(this.get(), function(index, el) {
      var $el = $(el);
      var $find = $el.children();

      $find.sort(function() {
        return 0.5 - Math.random();
      });

      $el.empty();
      $find.appendTo($el);
    });
  };
  $("#projects").shuffleChildren();
</script> -->

<!-- Project Cards Layout -->
<script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function () {
        $grid.masonry({percentPosition: true});
        $grid.masonry('layout');
    });


</script>

<!-- Enable Tooltips -->
<script type="text/javascript">
    $(function () {
        $('[data-toggle="tooltip"]').tooltip()
    })


</script>

<!-- Google Analytics -->
<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
            m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');


</script>
</body>
</html>
